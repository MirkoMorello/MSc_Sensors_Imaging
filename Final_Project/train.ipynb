{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omirako/.local/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dataset import SFMNNDataset\n",
    "from network import SFMNNEncoder\n",
    "from simulate import SFMNNSimulation\n",
    "from loss import SFMNNLoss\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading JSON files:   0%|          | 0/1350 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading JSON files: 100%|██████████| 1350/1350 [00:15<00:00, 86.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of elements loaded: 1350\n",
      "Total number of patches loaded: 60\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "data_folder = os.getenv(\"DATA_FOLDER\")\n",
    "\n",
    "with open(data_folder + \"simulation_sim_0_amb_0.json\") as f:\n",
    "    data = json.load(f)\n",
    "with open(data_folder + \"simulation_lookuptable.json\") as f:\n",
    "    lookup = json.load(f)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = SFMNNDataset(data_folder + 'simulation_lookuptable.json', 'output/', patch_size=5)\n",
    "\n",
    "# Load the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=True, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3620"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of spectral bands\n",
    "n_spectral_bands = len(dataset.get_wl())\n",
    "n_spectral_bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5, 3623)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H, W, C = dataset[0].shape\n",
    "H, W, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFMNNEncoder(\n",
      "  (input_norm): BatchNorm1d(3623, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=3623, out_features=4096, bias=True)\n",
      "    (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=8192, bias=True)\n",
      "    (5): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.1, inplace=False)\n",
      "    (8): Linear(in_features=8192, out_features=16384, bias=True)\n",
      "    (9): BatchNorm1d(16384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=16384, out_features=16384, bias=True)\n",
      "    (12): BatchNorm1d(16384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU()\n",
      "  )\n",
      "  (latent_proj): Linear(in_features=16384, out_features=32607, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network\n",
    "encoder = SFMNNEncoder(input_channels=C, latent_dim=C).to(device)\n",
    "\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(encoder.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = SFMNNLoss(torch.tensor(n_spectral_bands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n",
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n",
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n",
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n",
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n",
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n",
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n",
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n",
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n",
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n",
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n",
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n",
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n",
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n",
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n",
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n",
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n",
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n",
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n",
      "input: torch.Size([3, 5, 5, 3623])\n",
      "output: torch.Size([3, 5, 5, 9, 3623])\n"
     ]
    }
   ],
   "source": [
    "# test the network\n",
    "for i, data in enumerate(dataloader):\n",
    "    data = data.to(device)\n",
    "    print('input:', data.shape)\n",
    "    out = encoder(data)\n",
    "    print('output:', out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3620])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dataset.get_wl()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: torch.Size([3, 5, 5, 9, 3623])\n",
      "R shape: torch.Size([3, 5, 5, 3623])\n",
      "F shape: torch.Size([3, 5, 5, 3623])\n",
      "t_1 shape: torch.Size([3, 5, 5, 3623])\n",
      "t_2 shape: torch.Size([3, 5, 5, 3623])\n",
      "t_3 shape: torch.Size([3, 5, 5, 3623])\n",
      "t_4 shape: torch.Size([3, 5, 5, 3623])\n",
      "t_5 shape: torch.Size([3, 5, 5, 3623])\n",
      "t_6 shape: torch.Size([3, 5, 5, 3623])\n",
      "d_lambda shape: torch.Size([3, 5, 5])\n",
      "t1: torch.Size([3, 5, 5, 3623])\n",
      "t2: torch.Size([3, 5, 5, 3623])\n",
      "t3: torch.Size([3, 5, 5, 3623])\n",
      "t4: torch.Size([3, 5, 5, 3623])\n",
      "t5: torch.Size([3, 5, 5, 3623])\n",
      "t6: torch.Size([3, 5, 5, 3623])\n",
      "R: torch.Size([3, 5, 5, 3623])\n",
      "F: torch.Size([3, 5, 5, 3623])\n",
      "L_hr shape: torch.Size([3, 5, 5, 3623])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'conv1d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 37\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_lambda shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, d_lambda\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#print(\"d_lambda_avg shape:\", d_lambda_avg.shape)\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m sim_output \u001b[38;5;241m=\u001b[39m \u001b[43msimulation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_6\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimulation output type:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(simulation))\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# LOSS COMPUTATION\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[13], line 109\u001b[0m, in \u001b[0;36mSFMNNSimulation.forward\u001b[0;34m(self, t1, t2, t3, t4, t5, t6, R, F, delta_lambda, delta_sigma, E_s, cos_theta_s)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL_hr shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, L_hr\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# 2. Apply sensor simulation.\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m L_hyp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msensor_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL_hr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta_lambda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta_sigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m L_hyp\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[13], line 61\u001b[0m, in \u001b[0;36mHyPlantSensorSimulator.forward\u001b[0;34m(self, L_hr, delta_lambda, delta_sigma)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# 2. Spectral convolution along the spectral dimension.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Reshape L_hr so that the spectral dimension becomes the width of a 1-pixel–high image.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# L_hr: [B, C_hr, H, W] -> [B, H, W, C_hr] -> reshape to [B*H*W, 1, C_hr]\u001b[39;00m\n\u001b[1;32m     60\u001b[0m L_hr_reshaped \u001b[38;5;241m=\u001b[39m L_hr\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B \u001b[38;5;241m*\u001b[39m H \u001b[38;5;241m*\u001b[39m W, \u001b[38;5;241m1\u001b[39m, C_hr)\n\u001b[0;32m---> 61\u001b[0m L_blur \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m(L_hr_reshaped, kernel\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), padding\u001b[38;5;241m=\u001b[39mkernel_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Reshape back to [B, H, W, C_hr]\u001b[39;00m\n\u001b[1;32m     63\u001b[0m L_blur \u001b[38;5;241m=\u001b[39m L_blur\u001b[38;5;241m.\u001b[39mview(B, H, W, C_hr)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'conv1d'"
     ]
    }
   ],
   "source": [
    "simulation = SFMNNSimulation(torch.tensor(dataset.get_wl()).to(device))\n",
    "\n",
    "encoder.train()\n",
    "for data in tqdm(dataloader):\n",
    "    # MEASUREMENT STEP\n",
    "    data = data.to(device)\n",
    "\n",
    "    # ENCODING STEP\n",
    "    output = encoder(data)\n",
    "    print(\"Encoder output shape:\", output.shape)\n",
    "\n",
    "    # SIMULATION STEP\n",
    "    R           = output[..., 0, :]  # Reflectance parameters\n",
    "    F           = output[..., 1, :]  # Fluorescence parameters\n",
    "    t_1         = output[..., 2, :] \n",
    "    t_2         = output[..., 3, :]\n",
    "    t_3         = output[..., 4, :]\n",
    "    t_4         = output[..., 5, :]\n",
    "    t_5         = output[..., 6, :]\n",
    "    t_6         = output[..., 7, :]\n",
    "    #d_lambda    = output[..., 8, :]  # Expected shape: [B, N] or [B, H, W]\n",
    "    d_lambda    = output[..., 8, 0]  # Expected shape: [B, N] or [B, H, W]\n",
    "    #d_lambda_avg = d_lambda.mean(dim=(1,2,3)).view(d_lambda.shape[0], 1)\n",
    "    print(\"R shape:\", R.shape)\n",
    "    print(\"F shape:\", F.shape)\n",
    "    print(\"t_1 shape:\", t_1.shape)\n",
    "    print(\"t_2 shape:\", t_2.shape)\n",
    "    print(\"t_3 shape:\", t_3.shape)\n",
    "    print(\"t_4 shape:\", t_4.shape)\n",
    "    print(\"t_5 shape:\", t_5.shape)\n",
    "    print(\"t_6 shape:\", t_6.shape)\n",
    "    print(\"d_lambda shape:\", d_lambda.shape)\n",
    "    #print(\"d_lambda_avg shape:\", d_lambda_avg.shape)\n",
    "    \n",
    "\n",
    "    \n",
    "    sim_output = simulation(\n",
    "        t_1, t_2, t_3, t_4, t_5, t_6,\n",
    "        R,\n",
    "        F,\n",
    "        d_lambda,\n",
    "        torch.tensor(10, device=device),\n",
    "        torch.tensor(1, device=device),\n",
    "        torch.tensor(0, device=device)\n",
    "    )\n",
    "\n",
    "    print(\"Simulation output type:\", type(simulation))\n",
    "    \n",
    "    # LOSS COMPUTATION\n",
    "    loss = criterion(sim_output, data)\n",
    "    print(\"Loss:\", loss.item())\n",
    "\n",
    "    # BACKPROPAGATION\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: torch.Size([3, 5, 5, 9, 3623])\n",
      "Original d_lambda shape: torch.Size([3, 5, 5, 3623])\n",
      "Averaged d_lambda shape: torch.Size([3, 1])\n",
      "t1: torch.Size([3, 5, 5, 3623])\n",
      "t2: torch.Size([3, 5, 5, 3623])\n",
      "t3: torch.Size([3, 5, 5, 3623])\n",
      "t4: torch.Size([3, 5, 5, 3623])\n",
      "t5: torch.Size([3, 5, 5, 3623])\n",
      "t6: torch.Size([3, 5, 5, 3623])\n",
      "R: torch.Size([3, 5, 5, 3623])\n",
      "F: torch.Size([3, 5, 5, 3623])\n",
      "L_hr shape: torch.Size([3, 5, 5, 3623])\n",
      "Simulation output type: <class '__main__.SFMNNSimulation'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SFMNNLoss.forward() missing 3 required positional arguments: 'outputs', 'E_s', and 'cos_theta_s'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 165\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimulation output type:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(simulation))\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# LOSS COMPUTATION\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43msim_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# BACKPROPAGATION\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: SFMNNLoss.forward() missing 3 required positional arguments: 'outputs', 'E_s', and 'cos_theta_s'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F_func  # renamed to avoid shadowing by tensor 'F'\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- FourStreamSimulator ---\n",
    "class FourStreamSimulator(nn.Module):\n",
    "    def __init__(self, spectral_window=(750, 770), high_res=0.0055):\n",
    "        super().__init__()\n",
    "        self.register_buffer('lambda_hr', torch.arange(*spectral_window, high_res))\n",
    "        self.mu_f = 737.0  # Fluorescence peak wavelength\n",
    "\n",
    "    def forward(self, t1, t2, t3, t4, t5, t6, R, F, E_s, cos_theta_s):\n",
    "        print(f\"t1: {t1.shape}\")\n",
    "        print(f\"t2: {t2.shape}\")\n",
    "        print(f\"t3: {t3.shape}\")\n",
    "        print(f\"t4: {t4.shape}\")\n",
    "        print(f\"t5: {t5.shape}\")\n",
    "        print(f\"t6: {t6.shape}\")\n",
    "        print(f\"R: {R.shape}\")\n",
    "        print(f\"F: {F.shape}\")\n",
    "\n",
    "        # Compute product terms (following Table 3 from the paper)\n",
    "        t7 = t3 * t4\n",
    "        t8 = t3 * t6\n",
    "        t9 = t4 * t5\n",
    "        t10 = t4 * t2\n",
    "        t11 = t3 * t2\n",
    "\n",
    "        LTOA = t1 * t2 + (t1 * t8 * R + t9 * R + t10 * R + t11 * R + t6 * F + t7 * F) / (1 - t3 * R)\n",
    "        return LTOA\n",
    "\n",
    "# --- HyPlantSensorSimulator ---\n",
    "class HyPlantSensorSimulator(nn.Module):\n",
    "    def __init__(self, sensor_wavelengths, high_res=0.0055):\n",
    "        super().__init__()\n",
    "        # sensor_wavelengths: expected shape [n_sensor]\n",
    "        self.register_buffer('sensor_wavelengths', sensor_wavelengths)\n",
    "        self.high_res = high_res\n",
    "        self.register_buffer('wl_range', torch.tensor([sensor_wavelengths.min(), sensor_wavelengths.max()]))\n",
    "\n",
    "    def forward(self, L_hr, delta_lambda, delta_sigma):\n",
    "        \"\"\"\n",
    "        L_hr: [B, C_hr, H, W] where C_hr is the high-resolution spectral dimension.\n",
    "        delta_lambda: [B, 1] (one wavelength shift per image)\n",
    "        delta_sigma: scalar tensor\n",
    "        \"\"\"\n",
    "        B, C_hr, H, W = L_hr.shape\n",
    "\n",
    "        # 1. Create Gaussian SRF kernel\n",
    "        sigma = (0.27 + delta_sigma) * 2.3548  # convert FWHM to sigma\n",
    "        kernel_size = int(6 * sigma / self.high_res)\n",
    "        x = torch.linspace(-3 * sigma, 3 * sigma, kernel_size, device=L_hr.device)\n",
    "        kernel = torch.exp(-0.5 * (x / sigma) ** 2)\n",
    "        kernel /= kernel.sum()\n",
    "\n",
    "        # 2. Spectral convolution along the spectral dimension.\n",
    "        # Reshape L_hr so that the spectral dimension becomes the width of a 1-pixel–high image.\n",
    "        # L_hr: [B, C_hr, H, W] -> permute to [B, H, W, C_hr] then reshape to [B*H*W, 1, C_hr]\n",
    "        L_hr_reshaped = L_hr.permute(0, 2, 3, 1).reshape(B * H * W, 1, C_hr)\n",
    "        L_blur = F_func.conv1d(L_hr_reshaped, kernel.view(1, 1, -1), padding=kernel_size // 2)\n",
    "        # Compute the new spectral dimension from conv1d output:\n",
    "        new_C_hr = L_blur.shape[-1]\n",
    "        # Reshape back to [B, H, W, new_C_hr]\n",
    "        L_blur = L_blur.view(B, H, W, new_C_hr)\n",
    "\n",
    "        # 3. Wavelength shift and interpolation.\n",
    "        # sensor_wavelengths: [n_sensor]. After adding delta_lambda ([B,1]) and normalizing,\n",
    "        # we obtain coordinates for each image.\n",
    "        n_sensor = self.sensor_wavelengths.shape[0]\n",
    "        shifted_wl = self.sensor_wavelengths.unsqueeze(0) + delta_lambda  # [B, n_sensor]\n",
    "        normalized_wl = 2 * (shifted_wl - self.wl_range[0]) / (self.wl_range[1] - self.wl_range[0]) - 1  # [B, n_sensor]\n",
    "\n",
    "        # Expand these coordinates to every spatial location.\n",
    "        # normalized_wl_expanded: [B, H, W, n_sensor]\n",
    "        normalized_wl_expanded = normalized_wl.unsqueeze(1).unsqueeze(1).expand(B, H, W, n_sensor)\n",
    "        # Flatten spatial dimensions: [B*H*W, n_sensor]\n",
    "        normalized_wl_flat = normalized_wl_expanded.reshape(B * H * W, n_sensor)\n",
    "\n",
    "        # grid_sample expects a grid with last dimension 2 (for (y, x)).\n",
    "        # Here, we create dummy y coordinates (all zeros) because our \"image\" height is 1.\n",
    "        zero_y = torch.zeros_like(normalized_wl_flat)  # [B*H*W, n_sensor]\n",
    "        # Form grid: [B*H*W, n_sensor, 2] with (y, x) coordinates.\n",
    "        grid = torch.stack((zero_y, normalized_wl_flat), dim=-1)\n",
    "        # Reshape grid to [B*H*W, 1, n_sensor, 2] (grid_sample expects shape [N, H_out, W_out, 2])\n",
    "        grid = grid.view(B * H * W, 1, n_sensor, 2)\n",
    "\n",
    "        # Reshape L_blur to [B*H*W, 1, 1, new_C_hr] so that the \"image\" has height 1 and width = new_C_hr.\n",
    "        L_blur_reshaped = L_blur.view(B * H * W, 1, 1, new_C_hr)\n",
    "        # Sample using grid_sample.\n",
    "        L_sampled = F_func.grid_sample(L_blur_reshaped, grid, mode='bilinear', align_corners=False)\n",
    "        # L_sampled: [B*H*W, 1, 1, n_sensor]. Reshape to [B, H, W, n_sensor]\n",
    "        L_sampled = L_sampled.view(B, H, W, n_sensor)\n",
    "        # Permute to [B, n_sensor, H, W] (the desired output shape).\n",
    "        L_hyp = L_sampled.permute(0, 3, 1, 2)\n",
    "        return L_hyp\n",
    "\n",
    "# --- SFMNNSimulation ---\n",
    "class SFMNNSimulation(nn.Module):\n",
    "    def __init__(self, sensor_wavelengths):\n",
    "        super().__init__()\n",
    "        self.four_stream = FourStreamSimulator()\n",
    "        self.sensor_sim = HyPlantSensorSimulator(sensor_wavelengths)\n",
    "\n",
    "    def forward(self, t1, t2, t3, t4, t5, t6, R, F, delta_lambda, delta_sigma, E_s, cos_theta_s):\n",
    "        # 1. Compute high-resolution radiance.\n",
    "        L_hr = self.four_stream(t1, t2, t3, t4, t5, t6, R, F, E_s, cos_theta_s)\n",
    "        print(\"L_hr shape:\", L_hr.shape)\n",
    "        # 2. Apply sensor simulation.\n",
    "        L_hyp = self.sensor_sim(L_hr, delta_lambda, delta_sigma)\n",
    "        return L_hyp\n",
    "\n",
    "# --- Training Loop ---\n",
    "# Assumes that the following objects are defined and on the correct device:\n",
    "# - dataset: an instance of your dataset (with method get_wl())\n",
    "# - dataloader: a PyTorch DataLoader for your dataset\n",
    "# - encoder: your encoder network\n",
    "# - criterion: your loss function\n",
    "# - optimizer: the optimizer for your encoder parameters\n",
    "# - device: e.g., torch.device('cuda') or torch.device('cpu')\n",
    "\n",
    "# Initialize simulation with sensor wavelengths from the dataset.\n",
    "sensor_wl = torch.tensor(dataset.get_wl()).to(device)\n",
    "simulation = SFMNNSimulation(sensor_wl)\n",
    "\n",
    "encoder.train()\n",
    "for data in tqdm(dataloader):\n",
    "    # MEASUREMENT STEP\n",
    "    data = data.to(device)\n",
    "\n",
    "    # ENCODING STEP\n",
    "    output = encoder(data)\n",
    "    print(\"Encoder output shape:\", output.shape)\n",
    "\n",
    "    # Unpack encoder output.\n",
    "    # Assumes output has shape [B, 5, 5, N_params] with N_params >= 9.\n",
    "    R_param       = output[..., 0, :]  # Reflectance parameters\n",
    "    F_param       = output[..., 1, :]  # Fluorescence parameters\n",
    "    t_1           = output[..., 2, :] \n",
    "    t_2           = output[..., 3, :]\n",
    "    t_3           = output[..., 4, :]\n",
    "    t_4           = output[..., 5, :]\n",
    "    t_5           = output[..., 6, :]\n",
    "    t_6           = output[..., 7, :]\n",
    "    d_lambda      = output[..., 8, :]  # Expected shape: [B, 5, 5, 326070] (for example)\n",
    "    print(\"Original d_lambda shape:\", d_lambda.shape)\n",
    "\n",
    "    # Average d_lambda over dimensions 1, 2, and 3 to obtain shape [B, 1]\n",
    "    d_lambda_avg = d_lambda.mean(dim=(1, 2, 3)).view(d_lambda.shape[0], 1)\n",
    "    print(\"Averaged d_lambda shape:\", d_lambda_avg.shape)\n",
    "\n",
    "    # SIMULATION STEP\n",
    "    sim_output = simulation(\n",
    "        t_1, t_2, t_3, t_4, t_5, t_6,\n",
    "        R_param,\n",
    "        F_param,\n",
    "        d_lambda_avg,  # shape [B, 1]\n",
    "        torch.tensor(10, device=device),\n",
    "        torch.tensor(1, device=device),\n",
    "        torch.tensor(0, device=device)\n",
    "    )\n",
    "    print(\"Simulation output type:\", type(simulation))\n",
    "\n",
    "    # LOSS COMPUTATION\n",
    "    loss = criterion(sim_output, data)\n",
    "    print(\"Loss:\", loss.item())\n",
    "\n",
    "    # BACKPROPAGATION\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matlab-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
